{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhJ98OWOcWFw",
    "outputId": "fe02b780-ed90-49cc-d421-854e7527bb57",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyworld.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1XEo6WrJcXlE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from IPython import display\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import pyworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/LJSpeech-1.1/stats.csv', 'w') as statfile:\n",
    "    statfile.write(f'filename, energy, pitch\\n')\n",
    "    for path, dirs, files in os.walk('./data/LJSpeech-1.1/wavs'):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                samplerate, data = wavfile.read(path+'/'+file)\n",
    "                energy = np.linalg.norm(data)\n",
    "                data = data/np.max(np.abs(data))\n",
    "                \n",
    "                _pitch, t = pyworld.dio(data/32767, samplerate)\n",
    "                pitch = pyworld.stonemask(data/32767, _pitch, t, samplerate)\n",
    "                pitchmean = pitch[pitch.nonzero()].mean()\n",
    "                statfile.write(f'{file}, {energy}, {pitchmean}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv('./data/LJSpeech-1.1/stats.csv')\n",
    "text2file = pd.read_csv('./data/LJSpeech-1.1/metadata.csv', sep='|', header=None)\n",
    "text = process_text(train_config.data_path)\n",
    "ordered_energy_pitch = []\n",
    "tl = text2file[1].apply(lambda s:s.lower())\n",
    "i = 0\n",
    "for t in text:\n",
    "    i += 1\n",
    "    mel_gt_name = os.path.join(\n",
    "        train_config.mel_ground_truth, \"ljspeech-mel-%05d.npy\" % (i+1))\n",
    "    mel_gt_target = np.load(mel_gt_name)\n",
    "    t1 = t.lower()\n",
    "    vs = list(text2file[tl == t1[:-1]][0])\n",
    "    if vs:\n",
    "        name = vs[0] + '.wav'\n",
    "        cur = stats[stats['filename'] == name]\n",
    "        #energy = list(cur[' energy'])[0]\n",
    "        pitch = list(cur[' pitch'])[0]\n",
    "        ordered_energy_pitch.append((energy, pitch))\n",
    "    else:\n",
    "        ordered_energy_pitch.append((None, None))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/orderedstats.csv', 'w') as statfile:\n",
    "    statfile.write(f'energy,pitch\\n')\n",
    "    for energy, pitch in ordered_energy_pitch:\n",
    "        statfile.write(f'{energy},{pitch}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_stat = pd.read_csv('./data/orderedstats.csv')['pitch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_stat[pitch_stat != 'None'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MelSpectrogramConfig:\n",
    "    num_mels = 80\n",
    "\n",
    "@dataclass\n",
    "class FastSpeechConfig:\n",
    "    vocab_size = 300\n",
    "    max_seq_len = 3000\n",
    "\n",
    "    encoder_dim = 256\n",
    "    encoder_n_layer = 4\n",
    "    encoder_head = 2\n",
    "    encoder_conv1d_filter_size = 1024\n",
    "    \n",
    "    pitch_min = 127\n",
    "    pitch_max = 325\n",
    "    energy_min = 4\n",
    "    energy_max = 45\n",
    "    \n",
    "    n_bins = 256\n",
    "\n",
    "    decoder_dim = 256\n",
    "    decoder_n_layer = 4\n",
    "    decoder_head = 2\n",
    "    decoder_conv1d_filter_size = 1024\n",
    "\n",
    "    fft_conv1d_kernel = (9, 1)\n",
    "    fft_conv1d_padding = (4, 0)\n",
    "\n",
    "    duration_predictor_filter_size = 256\n",
    "    duration_predictor_kernel_size = 3\n",
    "    dropout = 0.1\n",
    "    \n",
    "    PAD = 0\n",
    "    UNK = 1\n",
    "    BOS = 2\n",
    "    EOS = 3\n",
    "\n",
    "    PAD_WORD = '<blank>'\n",
    "    UNK_WORD = '<unk>'\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    checkpoint_path = \"./model2_new\"\n",
    "    logger_path = \"./logger\"\n",
    "    mel_ground_truth = \"./mels\"\n",
    "    alignment_path = \"./alignments\"\n",
    "    data_path = './data/train.txt'\n",
    "    \n",
    "    wandb_project = 'fastspeech_example'\n",
    "    \n",
    "    text_cleaners = ['english_cleaners']\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    batch_size = 16\n",
    "    epochs = 2000\n",
    "    n_warm_up_step = 4000\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-6\n",
    "    grad_clip_thresh = 1.0\n",
    "    decay_step = [500000, 1000000, 2000000]\n",
    "\n",
    "    save_step = 3000\n",
    "    log_step = 5\n",
    "    clear_Time = 20\n",
    "\n",
    "    batch_expand_size = 32\n",
    "    \n",
    "\n",
    "mel_config = MelSpectrogramConfig()\n",
    "model_config = FastSpeechConfig()\n",
    "train_config = TrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import text_to_sequence\n",
    "\n",
    "\n",
    "def pad_1D(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = np.pad(x, (0, length - x.shape[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = np.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_1D_tensor(inputs, PAD=0):\n",
    "\n",
    "    def pad_data(x, length, PAD):\n",
    "        x_padded = F.pad(x, (0, length - x.shape[0]))\n",
    "        return x_padded\n",
    "\n",
    "    max_len = max((len(x) for x in inputs))\n",
    "    padded = torch.stack([pad_data(x, max_len, PAD) for x in inputs])\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def pad_2D(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        PAD = 0\n",
    "        if np.shape(x)[0] > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = np.shape(x)[1]\n",
    "        x_padded = np.pad(x, (0, max_len - np.shape(x)[0]),\n",
    "                          mode='constant',\n",
    "                          constant_values=PAD)\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = np.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(np.shape(x)[0] for x in inputs)\n",
    "        output = np.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def pad_2D_tensor(inputs, maxlen=None):\n",
    "\n",
    "    def pad(x, max_len):\n",
    "        if x.size(0) > max_len:\n",
    "            raise ValueError(\"not max_len\")\n",
    "\n",
    "        s = x.size(1)\n",
    "        x_padded = F.pad(x, (0, 0, 0, max_len-x.size(0)))\n",
    "        return x_padded[:, :s]\n",
    "\n",
    "    if maxlen:\n",
    "        output = torch.stack([pad(x, maxlen) for x in inputs])\n",
    "    else:\n",
    "        max_len = max(x.size(0) for x in inputs)\n",
    "        output = torch.stack([pad(x, max_len) for x in inputs])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_text(train_text_path):\n",
    "    with open(train_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = []\n",
    "        for line in f.readlines():\n",
    "            txt.append(line)\n",
    "\n",
    "        return txt\n",
    "\n",
    "def load_pitch_energy(train_pitch_energy):\n",
    "    with open(train_pitch_energy, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = []\n",
    "        for line in f.readlines():\n",
    "            if line != 'energy,pitch\\n':\n",
    "                txt.append(list(map(eval, line.split(','))))\n",
    "\n",
    "        return txt\n",
    "\n",
    "def get_data_to_buffer(train_config):\n",
    "    buffer = list()\n",
    "    text = process_text(train_config.data_path)\n",
    "    energy_pitch = load_pitch_energy('./data/orderedstats.csv')\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    for i in tqdm(range(len(text))):\n",
    "\n",
    "        mel_gt_name = os.path.join(\n",
    "            train_config.mel_ground_truth, \"ljspeech-mel-%05d.npy\" % (i+1))\n",
    "        mel_gt_target = np.load(mel_gt_name)\n",
    "        duration = np.load(os.path.join(\n",
    "            train_config.alignment_path, str(i)+\".npy\"))\n",
    "        character = text[i][0:len(text[i])-1]\n",
    "        character = np.array(\n",
    "            text_to_sequence(character, train_config.text_cleaners))\n",
    "\n",
    "        character = torch.from_numpy(character)\n",
    "        duration = torch.from_numpy(duration)\n",
    "        mel_gt_target = torch.from_numpy(mel_gt_target)\n",
    "        \n",
    "        energy, pitch = energy_pitch[i]\n",
    "        energy = torch.sqrt(torch.sum(torch.exp(mel_gt_target)**2))\n",
    "        energy_pitch[i][0] = energy\n",
    "        \n",
    "        if pitch is not None:\n",
    "            buffer.append({\"text\": character, \"duration\": duration,\n",
    "                           \"mel_target\": mel_gt_target,\n",
    "                           \"energy\": energy,\n",
    "                           \"pitch\": pitch})\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(\"cost {:.2f}s to load all data into buffer.\".format(end-start))\n",
    "\n",
    "    return buffer\n",
    "\n",
    "\n",
    "class BufferDataset(Dataset):\n",
    "    def __init__(self, buffer):\n",
    "        self.buffer = buffer\n",
    "        self.length_dataset = len(self.buffer)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.buffer[idx]\n",
    "\n",
    "\n",
    "def reprocess_tensor(batch, cut_list):    \n",
    "    texts = []\n",
    "    mel_targets = []\n",
    "    durations = []\n",
    "    pitches = []\n",
    "    energies = []\n",
    "    for ind in cut_list:\n",
    "        texts.append(batch[ind][\"text\"])\n",
    "        mel_targets.append(batch[ind][\"mel_target\"])\n",
    "        durations.append(batch[ind][\"duration\"])\n",
    "        pitches.append(batch[ind][\"pitch\"])\n",
    "        energies.append(batch[ind][\"energy\"])\n",
    "\n",
    "    length_text = np.array([])\n",
    "    for text in texts:\n",
    "        length_text = np.append(length_text, text.size(0))\n",
    "\n",
    "    src_pos = list()\n",
    "    max_len = int(max(length_text))\n",
    "    for length_src_row in length_text:\n",
    "        src_pos.append(np.pad([i+1 for i in range(int(length_src_row))],\n",
    "                              (0, max_len-int(length_src_row)), 'constant'))\n",
    "    src_pos = torch.from_numpy(np.array(src_pos))\n",
    "\n",
    "    length_mel = np.array(list())\n",
    "    for mel in mel_targets:\n",
    "        length_mel = np.append(length_mel, mel.size(0))\n",
    "\n",
    "    mel_pos = list()\n",
    "    max_mel_len = int(max(length_mel))\n",
    "    for length_mel_row in length_mel:\n",
    "        mel_pos.append(np.pad([i+1 for i in range(int(length_mel_row))],\n",
    "                              (0, max_mel_len-int(length_mel_row)), 'constant'))\n",
    "    mel_pos = torch.from_numpy(np.array(mel_pos))\n",
    "    \n",
    "    \n",
    "    texts = pad_1D_tensor(texts)\n",
    "    durations = pad_1D_tensor(durations)\n",
    "    mel_targets = pad_2D_tensor(mel_targets)\n",
    "    pitches = torch.tensor(pitches)\n",
    "    energies = torch.tensor(energies)\n",
    "\n",
    "    out = {\"text\": texts,\n",
    "           \"mel_target\": mel_targets,\n",
    "           \"duration\": durations,\n",
    "           \"mel_pos\": mel_pos,\n",
    "           \"src_pos\": src_pos,\n",
    "           \"mel_max_len\": max_mel_len,\n",
    "           \"pitch\": pitches,\n",
    "           \"energy\": energies,}\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_fn_tensor(batch):\n",
    "    len_arr = np.array([d[\"text\"].size(0) for d in batch])\n",
    "    index_arr = np.argsort(-len_arr)\n",
    "    batchsize = len(batch)\n",
    "    real_batchsize = batchsize // train_config.batch_expand_size\n",
    "\n",
    "    cut_list = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        cut_list.append(index_arr[i*real_batchsize:(i+1)*real_batchsize])\n",
    "\n",
    "    output = list()\n",
    "    for i in range(train_config.batch_expand_size):\n",
    "        output.append(reprocess_tensor(batch, cut_list[i]))\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = get_data_to_buffer(train_config)\n",
    "\n",
    "dataset = BufferDataset(buffer)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=train_config.batch_expand_size * train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_tensor,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itl = iter(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(itl)[0]\n",
    "batch['energy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have computer q,k,v matricies and we should calc attention score and calc weighted value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/20190325121034288.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: [ (batch_size * n_heads) x seq_len x hidden_size ]\n",
    "        \n",
    "        ### Your code here\n",
    "        \n",
    "        # attn: [ (batch_size * n_heads) x seq_len x seq_len ]\n",
    "        attn = torch.bmm(q, k.transpose(-1, -2))\n",
    "        attn /= self.temperature\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = torch.masked_fill(attn, mask, -torch.inf)\n",
    "\n",
    "        ### Your code here\n",
    "        \n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        # output: [ (batch_size * n_heads) x seq_len x hidden_size ]\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_attention = ScaledDotProductAttention(1)\n",
    "\n",
    "q = torch.randn(4 * 4, 8, 4)\n",
    "k = torch.randn(4 * 4, 8, 4)\n",
    "v = torch.randn(4 * 4, 8, 4)\n",
    "\n",
    "output, attn = dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на аттэншн мапы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(24, 4))\n",
    "for i, at in enumerate(attn[0:4]):\n",
    "    sns.heatmap(attn[i], ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/attention_picture.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут инпуты тремя линейными сломи преобразуем для подачи в selt attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            temperature=d_k**0.5) \n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "         # normal distribution initialization better than kaiming(default in pytorch)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0,\n",
    "                        std=np.sqrt(2.0 / (self.d_model + self.d_v))) \n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)  # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)  # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)  # (n*b) x lv x dv\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realization from Andrey Karpathy- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "- Flash Attention - https://arxiv.org/abs/2205.14135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Двухслойная сеть из 1d конволюций с релу в качестве активации. После идет драпаут и layer norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positionwise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use Conv1D\n",
    "        # position-wise\n",
    "        self.w_1 = nn.Conv1d(\n",
    "            d_in, d_hid, kernel_size=model_config.fft_conv1d_kernel[0], padding=model_config.fft_conv1d_padding[0])\n",
    "        # position-wise\n",
    "        self.w_2 = nn.Conv1d(\n",
    "            d_hid, d_in, kernel_size=model_config.fft_conv1d_kernel[1], padding=model_config.fft_conv1d_padding[1])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFTBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/fft.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совмещаем все вместе в один FFT(Feed Forward Transformer) BLock. Теперь можно стакать эти слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTBlock(torch.nn.Module):\n",
    "    \"\"\"FFT Block\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 d_inner,\n",
    "                 n_head,\n",
    "                 d_k,\n",
    "                 d_v,\n",
    "                 dropout=0.1):\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(\n",
    "            d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        \n",
    "        if non_pad_mask is not None:\n",
    "            enc_output *= non_pad_mask\n",
    "\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "intermediate_size = 64\n",
    "n_head = 4\n",
    "batch_size = 4\n",
    "seq_len = 12\n",
    "\n",
    "fft_block = FFTBlock(hidden_size, intermediate_size, n_head, hidden_size // n_head, hidden_size // n_head)\n",
    "\n",
    "inp_tensor = torch.rand(batch_size, seq_len, hidden_size, dtype=torch.float32)\n",
    "\n",
    "out_tensor = fft_block(inp_tensor)[0]\n",
    "\n",
    "assert inp_tensor.shape == out_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training tips - https://arxiv.org/pdf/1804.00247.pdf\n",
    "- Transformer without Tears - https://tnq177.github.io/data/transformers_without_tears.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Regulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alignment(base_mat, duration_predictor_output):\n",
    "    N, L = duration_predictor_output.shape\n",
    "    for i in range(N):\n",
    "        count = 0\n",
    "        for j in range(L):\n",
    "            for k in range(duration_predictor_output[i][j]):\n",
    "                base_mat[i][count+k][j] = 1\n",
    "            count = count + duration_predictor_output[i][j]\n",
    "    return base_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция которая делает бинарную матрицу для деблирование каждого мела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_alignment(\n",
    "    torch.zeros(1, 6, 3).numpy(),\n",
    "    torch.LongTensor([[1,2,3]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/duration_predictor.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim_1, dim_2):\n",
    "        super().__init__()\n",
    "        self.dim_1 = dim_1\n",
    "        self.dim_2 = dim_2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim_1, self.dim_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простая сетка с двумя конволюшналами и Линейным слоем агрегатом. Предсказываем тут длительность каждой фонемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\" Duration Predictor \"\"\"\n",
    "\n",
    "    def __init__(self, model_config: FastSpeechConfig):\n",
    "        super(DurationPredictor, self).__init__()\n",
    "\n",
    "        self.input_size = model_config.encoder_dim\n",
    "        self.filter_size = model_config.duration_predictor_filter_size\n",
    "        self.kernel = model_config.duration_predictor_kernel_size\n",
    "        self.conv_output_size = model_config.duration_predictor_filter_size\n",
    "        self.dropout = model_config.dropout\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.input_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.filter_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(self.conv_output_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        encoder_output = self.conv_net(encoder_output)\n",
    "            \n",
    "        out = self.linear_layer(encoder_output)\n",
    "        out = self.relu(out)\n",
    "        out = out.squeeze()\n",
    "        if not self.training:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariancePredictor(nn.Module):\n",
    "    \"\"\" Variance Predictor \"\"\"\n",
    "\n",
    "    def __init__(self, model_config: FastSpeechConfig):\n",
    "        super(VariancePredictor, self).__init__()\n",
    "\n",
    "        self.input_size = model_config.encoder_dim\n",
    "        self.filter_size = model_config.duration_predictor_filter_size\n",
    "        self.kernel = model_config.duration_predictor_kernel_size\n",
    "        self.conv_output_size = model_config.duration_predictor_filter_size\n",
    "        self.dropout = model_config.dropout\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.input_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            Transpose(-1, -2),\n",
    "            nn.Conv1d(\n",
    "                self.filter_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1\n",
    "            ),\n",
    "            Transpose(-1, -2),\n",
    "            nn.LayerNorm(self.filter_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(self.conv_output_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        encoder_output = self.conv_net(encoder_output)\n",
    "            \n",
    "        out = self.linear_layer(encoder_output)\n",
    "        out = self.relu(out)\n",
    "        out = out.squeeze()\n",
    "        if not self.training:\n",
    "            out = out.unsqueeze(0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthRegulator(nn.Module):\n",
    "    \"\"\" Length Regulator \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(LengthRegulator, self).__init__()\n",
    "        self.duration_predictor = VariancePredictor(model_config)\n",
    "\n",
    "    def LR(self, x, duration_predictor_output, mel_max_length=None):\n",
    "        expand_max_len = torch.max(\n",
    "            torch.sum(duration_predictor_output, -1), -1)[0]\n",
    "        alignment = torch.zeros(duration_predictor_output.size(0),\n",
    "                                expand_max_len,\n",
    "                                duration_predictor_output.size(-1)).numpy()\n",
    "        alignment = create_alignment(alignment,\n",
    "                                     duration_predictor_output.cpu().numpy())\n",
    "        alignment = torch.from_numpy(alignment).to(x.device)\n",
    "\n",
    "        output = alignment @ x\n",
    "        if mel_max_length:\n",
    "            output = F.pad(\n",
    "                output, (0, 0, 0, mel_max_length-output.size(1), 0, 0))\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, alpha=1.0, target=None, mel_max_length=None):\n",
    "        duration_predictor_output = self.duration_predictor(x)\n",
    "        if target is not None:\n",
    "            output = self.LR(x, target, mel_max_length)\n",
    "            return output, duration_predictor_output\n",
    "        else:\n",
    "            duration_predictor_output = (duration_predictor_output*alpha + 0.5).int()\n",
    "            output = self.LR(x, duration_predictor_output)\n",
    "            mel_pos = torch.stack([torch.Tensor([i+1 for i in range(output.size(1))])]).long().to(train_config.device)\n",
    "        \n",
    "            return output, mel_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceAdaptor(nn.Module):\n",
    "    \"\"\" Variance Adaptor \"\"\"\n",
    "    def __init__(self, model_config: FastSpeechConfig):\n",
    "        super(VarianceAdaptor, self).__init__()\n",
    "        self.length_regulator = LengthRegulator(model_config)\n",
    "        self.pitch_predictor = VariancePredictor(model_config)\n",
    "        self.energy_predictor = VariancePredictor(model_config)\n",
    "        \n",
    "        n_bins = model_config.n_bins\n",
    "        \n",
    "        self.energy_bins = nn.Parameter(\n",
    "                torch.linspace(model_config.energy_min, model_config.energy_max, n_bins - 1),\n",
    "                requires_grad=False,)\n",
    "        self.pitch_bins = nn.Parameter(\n",
    "                torch.linspace(model_config.pitch_min, model_config.pitch_max, n_bins - 1),\n",
    "                requires_grad=False,)\n",
    "        \n",
    "        self.energy_embedding = nn.Embedding(n_bins, model_config.encoder_dim)\n",
    "        self.pitch_embedding = nn.Embedding(n_bins, model_config.encoder_dim)\n",
    "        \n",
    "    def get_embedding(self, pred, emb, bins, x, target, control):\n",
    "        prediction = pred(x)\n",
    "        if target is not None:\n",
    "            embedding = emb(torch.bucketize(target, self.energy_bins))\n",
    "        else:\n",
    "            prediction = prediction * control\n",
    "            embedding = emb(torch.bucketize(prediction, self.energy_bins))\n",
    "        return prediction, embedding\n",
    "    \n",
    "    def get_energy_embedding(self, x, target, control):\n",
    "        return self.get_embedding(self.energy_predictor, self.energy_embedding, self.energy_bins, x, target, control)\n",
    "    \n",
    "    def get_pitch_embedding(self, x, target, control):\n",
    "        return self.get_embedding(self.pitch_predictor, self.pitch_embedding, self.pitch_bins, x, target, control)\n",
    "    \n",
    "    def forward(self, x, mel_max_length=None, length_target=None, alpha=1.0, pitch_target=None, energy_target=None, pitch_alpha=1.0, energy_alpha=1.0):\n",
    "        output, duration_predictor_output = self.length_regulator(x, alpha, length_target, mel_max_length)\n",
    "        \n",
    "        pitch_prediction, pitch_embedding = self.get_pitch_embedding(output, pitch_target, pitch_alpha)\n",
    "        output2 = output + pitch_embedding\n",
    "        energy_prediction, energy_embedding = self.get_energy_embedding(output, energy_target, energy_alpha)\n",
    "        output3 = output2 + energy_embedding\n",
    "        \n",
    "        return output3, duration_predictor_output, pitch_prediction, energy_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ad = VarianceAdaptor(model_config)\n",
    "inp_tensor = torch.rand(\n",
    "    2, # batch_size\n",
    "    12, #seq_len\n",
    "    model_config.encoder_dim,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "output, duration_predictor_output, pitch_prediction, energy_prediction = var_ad(inp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_prediction.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_predictor = VariancePredictor(model_config)\n",
    "\n",
    "inp_tensor = torch.rand(\n",
    "    2, # batch_size\n",
    "    12, #seq_len\n",
    "    model_config.encoder_dim,\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "dur_prediction = dur_predictor(inp_tensor)\n",
    "dur_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс который все объеденяет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final BLock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_pad_mask(seq):\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(model_config.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(model_config.PAD)\n",
    "    padding_mask = padding_mask.unsqueeze(\n",
    "        1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер и Декодер, берем FFT слои и цикликом по ним проходим. В энкодере эмбединги токенов и позишн эмбединги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.encoder_n_layer\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(\n",
    "            model_config.vocab_size,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, src_seq, src_pos, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n",
    "        non_pad_mask = get_non_pad_mask(src_seq)\n",
    "        \n",
    "        # -- Forward\n",
    "        enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attn_list += [enc_slf_attn]\n",
    "        \n",
    "\n",
    "        return enc_output, non_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        len_max_seq=model_config.max_seq_len\n",
    "        n_position = len_max_seq + 1\n",
    "        n_layers = model_config.decoder_n_layer\n",
    "\n",
    "        self.position_enc = nn.Embedding(\n",
    "            n_position,\n",
    "            model_config.encoder_dim,\n",
    "            padding_idx=model_config.PAD,\n",
    "        )\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([FFTBlock(\n",
    "            model_config.encoder_dim,\n",
    "            model_config.encoder_conv1d_filter_size,\n",
    "            model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            model_config.encoder_dim // model_config.encoder_head,\n",
    "            dropout=model_config.dropout\n",
    "        ) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_seq, enc_pos, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list = []\n",
    "\n",
    "        # -- Prepare masks\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=enc_pos, seq_q=enc_pos)\n",
    "        non_pad_mask = get_non_pad_mask(enc_pos)\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = enc_seq + self.position_enc(enc_pos)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn = dec_layer(\n",
    "                dec_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                dec_slf_attn_list += [dec_slf_attn]\n",
    "\n",
    "        return dec_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if max_len == None:\n",
    "        max_len = torch.max(lengths).item()\n",
    "\n",
    "    ids = torch.arange(0, max_len, 1, device=lengths.device)\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSpeech(nn.Module):\n",
    "    \"\"\" FastSpeech \"\"\"\n",
    "\n",
    "    def __init__(self, model_config):\n",
    "        super(FastSpeech, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(model_config)\n",
    "        self.variance_adaptor = VarianceAdaptor(model_config)\n",
    "        self.decoder = Decoder(model_config)\n",
    "\n",
    "        self.mel_linear = nn.Linear(model_config.decoder_dim, mel_config.num_mels)\n",
    "\n",
    "    def mask_tensor(self, mel_output, position, mel_max_length):\n",
    "        lengths = torch.max(position, -1)[0]\n",
    "        mask = ~get_mask_from_lengths(lengths, max_len=mel_max_length)\n",
    "        mask = mask.unsqueeze(-1).expand(-1, -1, mel_output.size(-1))\n",
    "        return mel_output.masked_fill(mask, 0.)\n",
    "\n",
    "    def forward(self, src_seq, src_pos, mel_pos=None, mel_max_length=None, length_target=None, alpha=1.0, pitch_target=None, energy_target=None, pitch_alpha=1.0, energy_alpha=1.0):\n",
    "        x, non_pad_mask = self.encoder(src_seq, src_pos)\n",
    "        \n",
    "        if self.training:\n",
    "            output, duration_predictor_output, pitch_prediction, energy_prediction = self.variance_adaptor(x, length_target=length_target, mel_max_length=mel_max_length, alpha=alpha, pitch_target=pitch_target, energy_target=pitch_target)\n",
    "            output = self.decoder(output, mel_pos)\n",
    "            \n",
    "            output = self.mask_tensor(output, mel_pos, mel_max_length)\n",
    "            output = self.mel_linear(output)\n",
    "            return output, duration_predictor_output, pitch_prediction, energy_prediction\n",
    "        else:\n",
    "            output, mel_pos, pitch_prediction, energy_prediction = self.variance_adaptor(x, alpha=alpha, pitch_alpha=pitch_alpha, energy_alpha=energy_alpha)\n",
    "            output = self.decoder(output, mel_pos)\n",
    "            output = self.mel_linear(output)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FastSpeechLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, mel, duration_predicted, pitch, energy, mel_target, duration_predictor_target, pitch_target, energy_target):\n",
    "        mel_loss = self.mse_loss(mel, mel_target)\n",
    "\n",
    "        duration_predictor_loss = self.l1_loss(duration_predicted,\n",
    "                                               duration_predictor_target.float())\n",
    "        \n",
    "        pitch_loss = self.l1_loss(torch.mean(pitch, -1), pitch_target)\n",
    "        energy_loss = self.l1_loss(torch.mean(energy, -1), energy_target)\n",
    "\n",
    "        return mel_loss, duration_predictor_loss, pitch_loss, energy_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler  import OneCycleLR\n",
    "from wandb_writer import WanDBWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastSpeech(model_config)\n",
    "model = model.to(train_config.device)\n",
    "model.load_state_dict(torch.load('./model2_new/checkpoint_123000.pth.tar', map_location='cuda:0')['model'])\n",
    "\n",
    "fastspeech_loss = FastSpeechLoss()\n",
    "current_step = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config.learning_rate,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9)\n",
    "\n",
    "scheduler = OneCycleLR(optimizer, **{\n",
    "    \"steps_per_epoch\": len(training_loader) * train_config.batch_expand_size,\n",
    "    \"epochs\": train_config.epochs,\n",
    "    \"anneal_strategy\": \"cos\",\n",
    "    \"max_lr\": train_config.learning_rate,\n",
    "    \"pct_start\": 0.1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batchs = next(iter(training_loader))\n",
    "db = batchs[0]\n",
    "character = db[\"text\"].long().to(train_config.device)\n",
    "mel_target = db[\"mel_target\"].float().to(train_config.device)\n",
    "duration = db[\"duration\"].int().to(train_config.device)\n",
    "mel_pos = db[\"mel_pos\"].long().to(train_config.device)\n",
    "src_pos = db[\"src_pos\"].long().to(train_config.device)\n",
    "max_mel_len = db[\"mel_max_len\"]\n",
    "pitch_target = db['pitch'].to(train_config.device)\n",
    "energy_target = db['energy'].to(train_config.device)\n",
    "pitch_target_expanded = torch.broadcast_to(torch.unsqueeze(pitch_target, 1), mel_pos.shape)\n",
    "energy_target_expanded = torch.broadcast_to(torch.unsqueeze(energy_target, 1), mel_pos.shape)\n",
    "\n",
    "# Forward\n",
    "mel_output, duration_predictor_output, pitch_prediction, energy_prediction = model(character,\n",
    "                                                                                  src_pos,\n",
    "                                                                                  mel_pos=mel_pos,\n",
    "                                                                                  mel_max_length=max_mel_len,\n",
    "                                                                                  length_target=duration,\n",
    "                                                                                  pitch_target=pitch_target_expanded,\n",
    "                                                                                  energy_target=energy_target_expanded,\n",
    "                                                                                  )\n",
    "mel_loss, duration_loss, pitch_loss, energy_loss = fastspeech_loss(mel_output,\n",
    "                                                                   duration_predictor_output,\n",
    "                                                                   pitch_prediction,\n",
    "                                                                   energy_prediction,\n",
    "                                                                   mel_target,\n",
    "                                                                   duration,\n",
    "                                                                   pitch_target,\n",
    "                                                                   energy_target)\n",
    "loss = mel_loss + duration_loss + pitch_loss + energy_loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WanDBWriter(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tqdm_bar = tqdm(total=train_config.epochs * len(training_loader) * train_config.batch_expand_size - current_step)\n",
    "\n",
    "\n",
    "for epoch in range(train_config.epochs):\n",
    "    for i, batchs in enumerate(training_loader):\n",
    "        # real batch start here\n",
    "        for j, db in enumerate(batchs):\n",
    "            current_step += 1\n",
    "            tqdm_bar.update(1)\n",
    "            \n",
    "            #logger.set_step(current_step)\n",
    "\n",
    "            # Get Data\n",
    "            character = db[\"text\"].long().to(train_config.device)\n",
    "            mel_target = db[\"mel_target\"].float().to(train_config.device)\n",
    "            duration = db[\"duration\"].int().to(train_config.device)\n",
    "            mel_pos = db[\"mel_pos\"].long().to(train_config.device)\n",
    "            src_pos = db[\"src_pos\"].long().to(train_config.device)\n",
    "            max_mel_len = db[\"mel_max_len\"]\n",
    "            pitch_target = db['pitch'].to(train_config.device)\n",
    "            energy_target = db['energy'].to(train_config.device)\n",
    "            pitch_target_expanded = torch.broadcast_to(torch.unsqueeze(pitch_target, 1), mel_pos.shape)\n",
    "            energy_target_expanded = torch.broadcast_to(torch.unsqueeze(energy_target, 1), mel_pos.shape)\n",
    "\n",
    "            # Forward\n",
    "            mel_output, duration_predictor_output, pitch_prediction, energy_prediction = model(character,\n",
    "                                                              src_pos,\n",
    "                                                              mel_pos=mel_pos,\n",
    "                                                              mel_max_length=max_mel_len,\n",
    "                                                              length_target=duration,\n",
    "                                                              pitch_target=pitch_target_expanded,\n",
    "                                                              energy_target=energy_target_expanded,\n",
    "                                                              )\n",
    "\n",
    "            # Calc Loss\n",
    "            mel_loss, duration_loss, pitch_loss, energy_loss = fastspeech_loss(mel_output,\n",
    "                                                                       duration_predictor_output,\n",
    "                                                                       pitch_prediction,\n",
    "                                                                       energy_prediction,\n",
    "                                                                       mel_target,\n",
    "                                                                       duration,\n",
    "                                                                       pitch_target,\n",
    "                                                                       energy_target)\n",
    "            total_loss = mel_loss + duration_loss + pitch_loss + energy_loss\n",
    "\n",
    "            # Logger\n",
    "            t_l = total_loss.detach().cpu().numpy()\n",
    "            m_l = mel_loss.detach().cpu().numpy()\n",
    "            d_l = duration_loss.detach().cpu().numpy()\n",
    "\n",
    "            logger.add_scalar(\"duration_loss\", d_l)\n",
    "            logger.add_scalar(\"mel_loss\", m_l)\n",
    "            logger.add_scalar(\"total_loss\", t_l)\n",
    "\n",
    "            # Backward\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Clipping gradients to avoid gradient explosion\n",
    "            nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), train_config.grad_clip_thresh)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "            if current_step % train_config.save_step == 0:\n",
    "                torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(\n",
    "                )}, os.path.join(train_config.checkpoint_path, 'checkpoint_%d.pth.tar' % current_step))\n",
    "                print(\"save model at step %d ...\" % current_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(train_config.checkpoint_path, 'checkpoint_%d.pth.tar' % current_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import waveglow\n",
    "import text\n",
    "import audio\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразовывать мел-спектрограмы в wav будем используя WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaveGlow = utils.get_WaveGlow()\n",
    "WaveGlow = WaveGlow.cuda()\n",
    "# model.load_state_dict(torch.load('../model_new/checkpoint_225000.pth.tar', map_location='cuda:0')['model'])\n",
    "#\n",
    "# model.load_state_dict(torch.load('./model2_new/checkpoint_123000.pth.tar', map_location='cuda:0')['model'])\n",
    "model = model.eval()\n",
    "model = model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def synthesis(model, text, alpha=1.0):\n",
    "    text = np.array(phn)\n",
    "    text = np.stack([text])\n",
    "    src_pos = np.array([i+1 for i in range(text.shape[1])])\n",
    "    src_pos = np.stack([src_pos])\n",
    "    sequence = torch.from_numpy(text).long().to(train_config.device)\n",
    "    src_pos = torch.from_numpy(src_pos).long().to(train_config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mel = model.forward(sequence, src_pos, alpha=alpha)\n",
    "    return mel[0].cpu().transpose(0, 1), mel.contiguous().transpose(1, 2)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    tests = [ \n",
    "        \"I am very happy to see you again!\",\n",
    "        \"Durian model is a very good speech synthesis!\",\n",
    "        \"When I was twenty, I fell in love with a girl.\",\n",
    "        \"I remove attention module in decoder and use average pooling to implement predicting r frames at once\",\n",
    "        \"You can not improve your past, but you can improve your future. Once time is wasted, life is wasted.\",\n",
    "        \"Death comes to all, but great achievements raise a monument which shall endure until the sun grows old.\"\n",
    "    ]\n",
    "    data_list = list(text.text_to_sequence(test, train_config.text_cleaners) for test in tests)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "data_list = get_data()\n",
    "for speed in [0.8, 1., 1.3]:\n",
    "    for i, phn in tqdm(enumerate(data_list)):\n",
    "        mel, mel_cuda = synthesis(model, phn, speed)\n",
    "        \n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        \n",
    "        audio.tools.inv_mel_spec(\n",
    "            mel, f\"results/s={speed}_{i}.wav\"\n",
    "        )\n",
    "        \n",
    "        waveglow.inference.inference(\n",
    "            mel_cuda, WaveGlow,\n",
    "            f\"results/s={speed}_{i}_waveglow.wav\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio.tools.inv_mel_spec(mel_output[0].T, f\"results/s={speed}_{i}.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерим звук с тремя разными скоростями используя возможности фастспича"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=0.8_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.0_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.3_5_waveglow.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=0.8_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.0_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(\"results/s=1.3_5.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FastSpeech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
